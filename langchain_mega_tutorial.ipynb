{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e28f373b",
   "metadata": {},
   "source": [
    "# LangChain Mega-Tutorial (Handsâ€‘On, Endâ€‘toâ€‘End)\n",
    "\n",
    "> Generated on 2025-09-22 05:36 â€” drop this into your environment and run cells topâ€‘toâ€‘bottom.\n",
    "\n",
    "This notebook is a **very descriptive, comprehensive LangChain tutorial** covering *most practical topics* endâ€‘toâ€‘end:\n",
    "- Installation & environment setup (Python, pip/uv, API keys)\n",
    "- Core concepts: **LLMs, ChatModels, Prompts, Output Parsers**\n",
    "- **LCEL (LangChain Expression Language)** & Runnables\n",
    "- **Chains** (sequential, map-reduce), **Routers**\n",
    "- **Tools** & **Function/Tool Calling**\n",
    "- **Agents** (ReAct, OpenAI Tools style), **Toolkits**\n",
    "- **Document Loaders**, **Text Splitters**, **Embeddings**, **VectorStores**\n",
    "- **Retrievers** & **RAG** (basic â†’ advanced), **Multiâ€‘query**, **HyDE**, **Reâ€‘ranking**\n",
    "- **Chat History & Memory** (buffer, summary, entity, vector)\n",
    "- **Callbacks, Tracing, Streaming, Async & Batching**\n",
    "- **Structured Output** (Pydantic/JSON), **Guardrails / Safety hooks**\n",
    "- **Evaluation** (unit tests for prompts/chains, Ragasâ€‘style ideas)\n",
    "- **Caching & Persistence**, **Checkpointers**\n",
    "- **LangGraph** basics for stateful graphs (agents & RAG workflows)\n",
    "- **Deployment**: **LangServe** quick start, FastAPI microservice pattern\n",
    "- **Observability**: **LangSmith** tracing + dataset evals (optional)\n",
    "- Tips, pitfalls, and production checklists\n",
    "\n",
    "> ðŸ“ **Note:** Many sections use optional providers (OpenAI, Anthropic, Groq, etc.). You can run with any supported model by swapping the model and client bits. Cells are written to be **providerâ€‘agnostic** where possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d507cb1",
   "metadata": {},
   "source": [
    "## 0) Prerequisites & Environment\n",
    "\n",
    "Run this cell to install the packages you need. If youâ€™re offline, skip now and run later in your own environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38dbf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're using a fresh environment, uncomment and run:\n",
    "# !pip install -U \"langchain>=0.2.16\" \"langchain-community>=0.2.11\" \"langchain-openai>=0.2.3\" #                \"faiss-cpu>=1.8.0\" \"pydantic>=2.6.0\" \"tiktoken\" \"python-dotenv\" \"fastapi\" \"uvicorn\" #                \"langchainhub\" \"beautifulsoup4\" \"pypdf\" \"unstructured\" \"lxml\" \"httpx\" \"aiohttp\"\n",
    "#\n",
    "# Optional (choose your provider integrations as needed):\n",
    "# !pip install -U \"openai>=1.44.0\" \"anthropic>=0.34.0\" \"groq>=0.9.0\" \"cohere>=5.5.8\"\n",
    "# !pip install -U \"ragas\" \"chromadb\" \"rank-bm25\" \"sentence-transformers\"\n",
    "#\n",
    "# For LangGraph/LangServe/LangSmith (optional):\n",
    "# !pip install -U \"langgraph>=0.2.38\" \"langserve>=0.3.0\" \"langsmith>=0.1.129\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a273acd2",
   "metadata": {},
   "source": [
    "### Environment Variables\n",
    "\n",
    "Create a `.env` file or export env vars before running provider cells:\n",
    "```bash\n",
    "export OPENAI_API_KEY=\"...\"         # or ANTHROPIC_API_KEY / GROQ_API_KEY / COHERE_API_KEY\n",
    "export LANGCHAIN_TRACING_V2=\"true\"  # optional (LangSmith)\n",
    "export LANGCHAIN_API_KEY=\"...\"      # optional (LangSmith)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4652b41a",
   "metadata": {},
   "source": [
    "## 1) Hello, LangChain â€” Minimal LLM & ChatModel\n",
    "\n",
    "We demonstrate both text LLMs and chat models using the **LangChain** wrappers. Swap `OpenAI` with your provider of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4fd8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# Minimal: chat completion\n",
    "chat = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)  # swap to your model\n",
    "resp = chat.invoke([HumanMessage(content=\"In one sentence, explain LangChain.\")] )\n",
    "print(resp.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dbbc9b",
   "metadata": {},
   "source": [
    "## 2) Prompt Templates & Output Parsers\n",
    "\n",
    "**PromptTemplate** lets you parameterize text; **ChatPromptTemplate** structures multiâ€‘turn prompts.  \n",
    "**Output parsers** help convert responses into Pythonic types (JSON, lists, Pydantic models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbdbdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Idea(BaseModel):\n",
    "    title: str = Field(..., description=\"Short idea title\")\n",
    "    rationale: str\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Idea)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=(\n",
    "        \"Generate a startup idea about {topic}.\"\n",
    "        \"{format_instructions}\"\n",
    "    ),\n",
    "    input_variables=[\"topic\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = prompt | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.4) | parser\n",
    "result = chain.invoke({\"topic\": \"personal finance for GenZ\"})\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4177ba93",
   "metadata": {},
   "source": [
    "## 3) LCEL (LangChain Expression Language) & Runnables\n",
    "\n",
    "LCEL composes components with the `|` operator. Everything becomes a **Runnable**; you can **invoke**, **batch**, and **stream**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619c309f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a concise assistant.\"),\n",
    "    (\"human\", \"Summarize in 15 words: {text}\")\n",
    "])\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "to_upper = RunnableLambda(lambda s: s.upper())\n",
    "\n",
    "lcel_chain = prompt | model | StrOutputParser() | to_upper\n",
    "lcel_chain.invoke({\"text\": \"LangChain helps you build LLM apps by composing components cleanly.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e69ec91",
   "metadata": {},
   "source": [
    "## 4) Chains: Sequential, Mapâ€‘Reduce, Router\n",
    "\n",
    "Chains coordinate multiple steps. Below: a simple sequential chain, then a toy mapâ€‘reduce summarizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570d5349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents.base import BaseCombineDocumentsChain\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "docs = [Document(page_content=s) for s in [\n",
    "    \"LangChain is a framework to build with LLMs.\",\n",
    "    \"It offers integrations for prompts, models, memory, tools, and more.\",\n",
    "    \"LCEL composes components using a pipe operator.\"\n",
    "]]\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "map_reduce: BaseCombineDocumentsChain = load_summarize_chain(model, chain_type=\"map_reduce\")\n",
    "map_reduce.run(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe32b8a",
   "metadata": {},
   "source": [
    "## 5) Tools & Function/Tool Calling\n",
    "\n",
    "Expose Python functions as **tools**; modern models can decide when to call them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59484b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Return sunny/foggy/rainy (toy).\"\"\"\n",
    "    return f\"Weather in {city}: sunny 27Â°C (demo)\"\n",
    "\n",
    "tools = [get_weather]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "executor.invoke({\"input\": \"What is the weather in Jaipur? Should I carry sunglasses?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819e7f6b",
   "metadata": {},
   "source": [
    "## 6) Agents: ReAct & OpenAI Tools Style\n",
    "\n",
    "**Agents** reason about which tools to use and in what order. Use **ReAct** for traceable reasoning or **Tools style** for builtâ€‘in function calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5c442a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReAct-style agent example (uses zero-shot-react-description)\n",
    "from langchain.agents import load_tools, initialize_agent, AgentType\n",
    "\n",
    "search_tools = []  # e.g., load_tools([\"ddg-search\"]) if installed & configured\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "react_agent = initialize_agent(\n",
    "    tools + search_tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
    ")\n",
    "react_agent.run(\"Plan a 1-day food crawl in Lucknow. Use get_weather if helpful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243777bc",
   "metadata": {},
   "source": [
    "## 7) Documents, Loaders & Text Splitters\n",
    "\n",
    "Load data from the web, PDFs, or files; split into chunks for embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb5f194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = WebBaseLoader(\"https://www.langchain.com/\")  # try on any public page\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)\n",
    "len(chunks), chunks[0].page_content[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18058841",
   "metadata": {},
   "source": [
    "## 8) Embeddings & VectorStores (FAISS)\n",
    "\n",
    "Create embeddings for chunks and build a FAISS index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe30039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "emb = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vs = FAISS.from_documents(chunks[:20], embedding=emb)  # index a subset for speed\n",
    "retriever = vs.as_retriever(search_kwargs={\"k\": 4})\n",
    "retriever.get_relevant_documents(\"What is LangChain for?\")[0].page_content[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d4810b",
   "metadata": {},
   "source": [
    "## 9) RAG: Basic Retrieverâ€‘Augmented Generation\n",
    "\n",
    "Classic **RAG** = Retrieve topâ€‘k docs â†’ stuff into prompt â†’ generate answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c989b3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever\n",
    ")\n",
    "qa.invoke({\"query\": \"Summarize the purpose of LangChain and LCEL.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d099c577",
   "metadata": {},
   "source": [
    "## 10) RAG: Advanced Techniques (Multiâ€‘Query, HyDE, Reâ€‘ranking)\n",
    "\n",
    "- **Multiâ€‘Query**: generate diverse queries to improve recall  \n",
    "- **HyDE**: create a hypothetical answer and embed it to retrieve  \n",
    "- **Reâ€‘ranking**: score retrieved docs, keep the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b109d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "mqr = MultiQueryRetriever.from_llm(retriever=vs.as_retriever(), llm=ChatOpenAI(model=\"gpt-4o-mini\"))\n",
    "docs = mqr.get_relevant_documents(\"How does LCEL help composition?\")\n",
    "[ (i, d.metadata.get(\"source\",\"\"), d.page_content[:100]) for i, d in enumerate(docs) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e569a25",
   "metadata": {},
   "source": [
    "## 11) Conversation Memory (Buffer, Summary, Entity, Vector)\n",
    "\n",
    "Use memory to maintain context across turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b936367",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "conv = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
    "\n",
    "conv.predict(input=\"Hi, I'm planning a trip to Kerala.\")\n",
    "conv.predict(input=\"I love beaches and budget is medium.\")\n",
    "conv.predict(input=\"Remind me: what did I say I like?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0128cdd6",
   "metadata": {},
   "source": [
    "## 12) Callbacks, Tracing, Streaming & Async/Batching\n",
    "\n",
    "Callbacks provide hooks (start/end, token events). Enable **LangSmith** for tracing. Streaming yields tokens incrementally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387afe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "streaming_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
    "streaming_llm.invoke(\"Stream a haiku about monsoons in Pune.\")\n",
    "\n",
    "# Async batching demo\n",
    "async def a_batch(prompts):\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    results = await llm.abatch(prompts)\n",
    "    return [r.content for r in results]\n",
    "\n",
    "asyncio.run(a_batch([\"One fact about mangoes.\", \"One fact about coconuts.\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1930724f",
   "metadata": {},
   "source": [
    "## 13) Structured Output & Guardrails\n",
    "\n",
    "Use Pydantic schemas for reliable JSON. Add simple content filters/validators as pre/post hooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dd15bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, field_validator\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "class Place(BaseModel):\n",
    "    name: str\n",
    "    city: str\n",
    "    budget: float\n",
    "    @field_validator(\"budget\")\n",
    "    def non_negative(cls, v):\n",
    "        if v < 0: raise ValueError(\"Budget must be non-negative\")\n",
    "        return v\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Place)\n",
    "\n",
    "system_guard = RunnablePassthrough()  # stub; insert moderation calls here if you have them\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Recommend a venue as JSON.{fmt} User: {wish}\",\n",
    "    input_variables=[\"wish\"],\n",
    "    partial_variables={\"fmt\": parser.get_format_instructions()}\n",
    ")\n",
    "chain = system_guard | (prompt | ChatOpenAI(model=\"gpt-4o-mini\") | parser)\n",
    "chain.invoke({\"wish\": \"I need a party place in Jaipur under $200\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a914b0",
   "metadata": {},
   "source": [
    "## 14) Evaluation Basics\n",
    "\n",
    "Write quick tests for prompts/chains. For larger evals use **LangSmith datasets** or **Ragas**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af8786e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_non_empty(s: str):\n",
    "    assert isinstance(s, str) and len(s.strip()) > 0, \"Empty output\"\n",
    "\n",
    "out = (ChatOpenAI(model=\"gpt-4o-mini\") | StrOutputParser()).invoke(\"Return a single word: hello\")\n",
    "assert_non_empty(out)\n",
    "print(\"âœ… Simple eval passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c464669c",
   "metadata": {},
   "source": [
    "## 15) Caching & Persistence\n",
    "\n",
    "Cache LLM calls (inâ€‘memory, SQLite, Redis, etc.) to speed up development and cut costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5ca858",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import InMemoryCache\n",
    "from langchain.globals import set_llm_cache\n",
    "\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "print(llm.invoke(\"Say 'cached hello'\").content)  # first\n",
    "print(llm.invoke(\"Say 'cached hello'\").content)  # served from cache (if enabled/provider supports)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf21a57c",
   "metadata": {},
   "source": [
    "## 16) LangGraph (Optional): Stateful Graphs for Agents & RAG\n",
    "\n",
    "Use **LangGraph** to build deterministic graphs around LLM steps, tools, and state. Below is a tiny graph with two nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a3541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal LangGraph example (ensure langgraph is installed)\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "class State(TypedDict):\n",
    "    message: str\n",
    "\n",
    "def node_upper(state: State):\n",
    "    return {\"message\": state[\"message\"].upper()}\n",
    "\n",
    "def node_suffix(state: State):\n",
    "    return {\"message\": state[\"message\"] + \" âœ…\"}\n",
    "\n",
    "graph = StateGraph(State)\n",
    "graph.add_node(\"upper\", node_upper)\n",
    "graph.add_node(\"suffix\", node_suffix)\n",
    "graph.set_entry_point(\"upper\")\n",
    "graph.add_edge(\"upper\", \"suffix\")\n",
    "graph.add_edge(\"suffix\", END)\n",
    "\n",
    "app = graph.compile()\n",
    "app.invoke({\"message\": \"hello graph\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd38eea2",
   "metadata": {},
   "source": [
    "## 17) Deployment with LangServe (Optional)\n",
    "\n",
    "Expose your chain with **LangServe**. Start FastAPI and call your chain over HTTP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d40037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py (example)\n",
    "# from fastapi import FastAPI\n",
    "# from langserve import add_routes\n",
    "# from langchain.prompts import ChatPromptTemplate\n",
    "# from langchain_openai import ChatOpenAI\n",
    "#\n",
    "# app = FastAPI()\n",
    "# chain = ChatPromptTemplate.from_messages([(\"human\", \"{input}\")]) | ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "# add_routes(app, chain, path=\"/chain\")\n",
    "#\n",
    "# if __name__ == \"__main__\":\n",
    "#     import uvicorn\n",
    "#     uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "#\n",
    "# Run server:\n",
    "# uvicorn main:app --reload --port 8000\n",
    "#\n",
    "# Then call:\n",
    "# curl -X POST http://localhost:8000/chain/invoke -H \"Content-Type: application/json\" -d '{\"input\": {\"input\":\"Hello\"}}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63e9b53",
   "metadata": {},
   "source": [
    "## 18) LangSmith Observability (Optional)\n",
    "\n",
    "Turn on tracing and link runs to datasets and evals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65424046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export LANGCHAIN_TRACING_V2=true\n",
    "# export LANGCHAIN_API_KEY=...\n",
    "#\n",
    "# from langsmith import Client\n",
    "# client = Client()\n",
    "# dataset = client.create_dataset(\"my-eval-set\", description=\"Toy Q/A\")\n",
    "# client.create_example(inputs={\"q\":\"What is LCEL?\"}, outputs={\"a\":\"...\"}, dataset_id=dataset.id)\n",
    "# # Run your chain across dataset via LangSmith UI or SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b849809",
   "metadata": {},
   "source": [
    "## 19) Production Checklist & Pitfalls\n",
    "\n",
    "- **Determinism**: fix temperatures, seed where supported; add unit tests for prompts.\n",
    "- **Resilience**: timeouts, retries, fallbacks (secondary model or cached answer).\n",
    "- **Guardrails**: input validation, output schemas, allow/deny lists, PII scrubbing.\n",
    "- **Observability**: tracing, logs, metrics; capture prompts+latency+cost.\n",
    "- **Cost control**: batching, compression/summarization, smaller contexts.\n",
    "- **Data**: deduplicate, chunk sensibly, consider reâ€‘ranking and hybrid search.\n",
    "- **Ops**: version datasets, prompts, chains; blue/green deploys; rollbacks.\n",
    "- **Security**: never log secrets; sign tool calls; sandbox untrusted code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b242ad",
   "metadata": {},
   "source": [
    "## 20) Appendix: Swapping Providers Quickly\n",
    "\n",
    "Replace `ChatOpenAI` with e.g. `from langchain_groq import ChatGroq` or `from langchain_anthropic import ChatAnthropic`, update env vars, and keep the rest of the pipeline identical."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
